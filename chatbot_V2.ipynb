{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_Z_awNlHEQN5",
    "outputId": "71461582-6e44-4bb4-9202-7b5ab878d0a1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install langchain unstructured openai chromadb Cython tiktoken pypdf lark langchain_openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Stage:\n",
    "![Retrieval Stage](./images/RS.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "bBAUJuuuEbT3"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import pandas as pd\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain_openai import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_openai  import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from typing import List\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chatbot Memory Class:\n",
    " - ConversationSummaryBufferMemory: It keeps a buffer of recent interactions in memory, but when the interactions' number of tokens exceeds the \"max_token_limit\", it will summarize the chat history using the LLM.\n",
    " - A store function, to save the user input and the final output\n",
    " - A is_empty function, to check if the memory is empty\n",
    " - A load function, to get the chat history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "5gl5vwD7Eods"
   },
   "outputs": [],
   "source": [
    "class chatbot_memo:\n",
    "  def __init__(self, max_token_limit=100):\n",
    "    self.llm_memo = OpenAI()\n",
    "    self.memory = ConversationSummaryBufferMemory(\n",
    "        llm = self.llm_memo,\n",
    "        memory_key=\"chat_history\",\n",
    "        input_key=\"question\",\n",
    "        max_token_limit=max_token_limit,\n",
    "    )\n",
    "\n",
    "  def store(self, question, answer):\n",
    "    self.memory.save_context({\"question\": question}, {\"output\": answer})\n",
    "\n",
    "  def is_empty(self):\n",
    "    if len(self.memory.load_memory_variables({})['chat_history']) > 0:\n",
    "      return False\n",
    "    else:\n",
    "      return True\n",
    "\n",
    "  def load(self):\n",
    "    conversation = self.memory.load_memory_variables({})['chat_history']\n",
    "\n",
    "    return conversation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question or not Chain Class:\n",
    " - It returns a dictionary (JSON format) informing whether the user input is a question about space, sun, or sunspots.\n",
    " - We prepare the format for the output using a class inherited from the \"langchain_core.pydantic_v1.BaseModel\" class.\n",
    " - If the user input is not a question about space, sun, or sunspots, it will generate a reply for it.\n",
    " - A run chain function, to get the output of the chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "j9HyfvjFEbRw"
   },
   "outputs": [],
   "source": [
    "class question_or_not_json_form(BaseModel):\n",
    "    is_question: bool = Field(description=\"the message is question about space, sun, sunspots or not\")\n",
    "    message: str = Field(description=\"the input message\")\n",
    "    response: str = Field(description=\"the response for the message if it's not a question about space, sun, sunspots, type None if it's a question about space, sun, sunspots\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "UJjPUH5EEbPg"
   },
   "outputs": [],
   "source": [
    "class question_or_not_chain:\n",
    "\n",
    "  def __init__(self, llm_name = \"gpt-3.5-turbo\"):\n",
    "    self.llm_name = llm_name\n",
    "    self.llm = ChatOpenAI(model_name=self.llm_name, temperature=0)\n",
    "    self.parser = JsonOutputParser(pydantic_object=question_or_not_json_form)\n",
    "    self.prompt = PromptTemplate(\n",
    "        template=\"you are a chat bot and your name is Morfy, you have to check if the message is a question about space, sun, sunspots or not, if not write a response for it.\\n{format_instructions}\\n{message}\\n\",\n",
    "        input_variables=[\"message\"],\n",
    "        partial_variables={\"format_instructions\": self.parser.get_format_instructions()},\n",
    "    )\n",
    "    self.chain = self.prompt | self.llm | self.parser\n",
    "\n",
    "  def run_chain(self, message):\n",
    "      results = self.chain.invoke({\"message\": message})\n",
    "\n",
    "      return results\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paraphrasing Chain Class:\n",
    " - The chain returns the human new question as a stand-alone question if it was a follow-up question.\n",
    " - If the memory is empty, this chain will not run.\n",
    " - If the new question is not a follow-up question, it should return it as it is.\n",
    " - A run chain function, to get the output of the chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "cCNfQnmEEbNB"
   },
   "outputs": [],
   "source": [
    "class paraphrasing_chain:\n",
    "\n",
    "  def __init__(self, llm_name = \"gpt-3.5-turbo\"):\n",
    "\n",
    "    self.llm_name = llm_name\n",
    "\n",
    "    self.llm = ChatOpenAI(model_name=self.llm_name, temperature=0)\n",
    "\n",
    "    self.template = \"\"\"system:Use the chat history to paraphrase the human new question as stand alone question, if it's not a follow up question just pass it as it is to the output.\\n\n",
    "\n",
    "    \\n\n",
    "    the chat history:\\n\n",
    "    {chat_history}\n",
    "\n",
    "    \\n\n",
    "    human new question: {question}\"\"\"\n",
    "\n",
    "    self.paraphrasing_prompt = PromptTemplate(input_variables=[\"chat_history\", \"question\"], template=self.template)\n",
    "    self.chain = self.paraphrasing_prompt  | self.llm\n",
    "\n",
    "  def run_chain(self, message, memory):\n",
    "    results = self.chain.invoke({\"question\": message, \"chat_history\":memory})\n",
    "\n",
    "    return results.content\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documents Retriever Class:\n",
    " - It uses the OpenAIEmbeddings, to get the most relevant documents from the preprocessed documents in the database in the \"persist_directory\".\n",
    " - A retrieve function, to get the most \"K\" relevant document using the \"search_type\" based on the \"score_threshold\", then it formats the data to be passed to the answer chain.\n",
    " - Please check the preprocessing notebook to learn more about this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "KLQbxb4EEbKx"
   },
   "outputs": [],
   "source": [
    "class docs_retriever:\n",
    "\n",
    "  def __init__(self, persist_directory=\"./chroma\", search_type=\"similarity_score_threshold\", score_threshold = 0.5, k = 4):\n",
    "    self.embedding = OpenAIEmbeddings()\n",
    "    self.vectordb = Chroma(persist_directory=persist_directory, embedding_function=self.embedding)\n",
    "    self.retriever = self.vectordb.as_retriever(search_type = search_type, search_kwargs={\"score_threshold\": score_threshold , \"k\": k})\n",
    "\n",
    "  def retrieve(self, question):\n",
    "    docs = self.retriever.get_relevant_documents(question)\n",
    "    context = \"\"\n",
    "    for i, doc in enumerate(docs):\n",
    "      try:\n",
    "        doc_content = doc.page_content.replace(\"__________\", \"\")\n",
    "      except:\n",
    "        doc_content = doc.page_content\n",
    "      context = context + (\"document \" + str(i+1) + \":\\n\") + doc_content + \"\\n\"\n",
    "\n",
    "    return context\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer Chain Class:\n",
    " - It will use the LLM to generate an answer using the context from \"docs_retriever\".\n",
    " - It will return the media ID if it exists.\n",
    " - The output is a dictionary, containing the answer with images and tables IDs.\n",
    " - The media retrieve feature is based on dictionary-like strings in the most relevant documents (to learn more please check the readme file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "hVzh6dTjEbId"
   },
   "outputs": [],
   "source": [
    "class image_atrris(BaseModel):\n",
    "    image_id: str = Field(description=\"the id for image varibale from the most relevent document\")\n",
    "    image_description: str = Field(description=\"the description for image varibale from the most relevent document\")\n",
    "\n",
    "class table_atrris(BaseModel):\n",
    "    table_id: str = Field(description=\"the id for table varibale from the most relevent document\")\n",
    "    table_description: str = Field(description=\"the description for table varibale from the most relevent document\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "30tML-Y5EbF-"
   },
   "outputs": [],
   "source": [
    "class get_answer(BaseModel):\n",
    "    has_media: bool = Field(description=\"does the most relevent document have images or tables variables\")\n",
    "    answer: str = Field(description=\"the answer for the question\")\n",
    "    images: List[image_atrris] = Field(description=\"a list of images variables attributes if existed in the most relevent document\")\n",
    "    tables: List[table_atrris] = Field(description=\"a list of tables variables attributes if existed in the most relevent document\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "KVW_wiprEbDw"
   },
   "outputs": [],
   "source": [
    "class answer_chain:\n",
    "  def __init__(self):\n",
    "\n",
    "    self.llm = OpenAI()\n",
    "    self.parser = JsonOutputParser(pydantic_object=get_answer)\n",
    "\n",
    "    self.prompt = PromptTemplate(\n",
    "        template=\"Given the following extracted documents and a question, create a final answer.\\nthe documents:\\n{context}.\\n{format_instructions}\\n human question:{message}\\n\",\n",
    "        input_variables=[\"message\", \"context\"],\n",
    "        partial_variables={\"format_instructions\": self.parser.get_format_instructions()},\n",
    "        )\n",
    "\n",
    "    self.chain = self.prompt | self.llm | self.parser\n",
    "\n",
    "  def run_chain(self, message, context):\n",
    "    results = self.chain.invoke({\"context\":context, \"message\": message})\n",
    "\n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Media Retriever Class:\n",
    " - Takes the ID and the media type and uses a CSV file to get its path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "2Ki2hc3VUmOF"
   },
   "outputs": [],
   "source": [
    "class media_retriever:\n",
    "  def __init__(self, media_path=\"./images_tables\", IDs_table_path = \"./images_tables/final-tokens.csv\"):\n",
    "    self.ID_table = pd.read_csv(IDs_table_path)\n",
    "    self.media_path = media_path\n",
    "\n",
    "  def get_media(self, ID, m_type):\n",
    "    int_ID = int(ID[ID.find(\" \")+1::])\n",
    "    name = self.ID_table.loc[(self.ID_table['ID'] == int_ID) & (self.ID_table['type'] == m_type)].iloc[0][\"name\"]\n",
    "    path = self.media_path +\"/\" + m_type + \"s/\" + name\n",
    "\n",
    "    return path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting It All Together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "KGcSPMY1EbBX"
   },
   "outputs": [],
   "source": [
    "class QA_BO_chatbot:\n",
    "\n",
    "  def __init__(self,max_token_limit=100, llm_name = \"gpt-3.5-turbo\", persist_directory=\"./chroma\", search_type=\"similarity_score_threshold\", score_threshold = 0.5, k = 4, media_path=\"./images_tables\", IDs_table_path = \"./images_tables/final-tokens.csv\"):\n",
    "    self.bot_memo = chatbot_memo(max_token_limit)\n",
    "    self.question_or_not = question_or_not_chain(llm_name)\n",
    "    self.paraphrasing = paraphrasing_chain(llm_name)\n",
    "    self.retriever = docs_retriever(persist_directory, search_type, score_threshold, k)\n",
    "    self.answerer = answer_chain()\n",
    "    self.MediaRetriever = media_retriever(media_path, IDs_table_path)\n",
    "\n",
    "  def run_chain(self, message):\n",
    "    imgs = []\n",
    "    tables = []\n",
    "    S1 = self.question_or_not.run_chain(message)\n",
    "\n",
    "    if (S1[\"is_question\"]):\n",
    "      if (self.bot_memo.is_empty()):\n",
    "        memo = self.bot_memo.load()\n",
    "        paraphrased_question = self.paraphrasing.run_chain(message, memo)\n",
    "        context =  self.retriever.retrieve(paraphrased_question)\n",
    "        answerer_output = self.answerer.run_chain(paraphrased_question, context)\n",
    "      else:\n",
    "        context =  self.retriever.retrieve(message)\n",
    "        answerer_output = self.answerer.run_chain(message, context)\n",
    "\n",
    "      if (answerer_output[\"has_media\"]):\n",
    "        if len(answerer_output[\"images\"]) > 0:\n",
    "          for img in answerer_output[\"images\"]:\n",
    "            path = self.MediaRetriever.get_media(img[\"image_id\"], \"image\")\n",
    "            desc = img[\"image_description\"]\n",
    "            imgs.append({\"path\":path, \"description\": desc})\n",
    "\n",
    "        if len(answerer_output[\"tables\"]) > 0:\n",
    "          for table in answerer_output[\"tables\"]:\n",
    "            path = self.MediaRetriever.get_media(table[\"table_id\"], \"table\")\n",
    "            desc = table[\"table_description\"]\n",
    "            tables.append({\"path\":path, \"description\": desc})\n",
    "\n",
    "      final_output = {\"input\": message, \"response\": answerer_output[\"answer\"], \"has_media\": answerer_output[\"has_media\"], \"images\": imgs, \"tables\": tables}\n",
    "    else:\n",
    "       final_output = {\"input\": message, \"response\": S1[\"response\"], \"has_media\": False, \"images\": [], \"tables\": []}\n",
    "\n",
    "    self.bot_memo.store(message, final_output[\"response\"])\n",
    "\n",
    "    return final_output\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing The Chatbot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "vPW2xAT5Ea_D"
   },
   "outputs": [],
   "source": [
    "final_bot = QA_BO_chatbot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "id": "P_0kBZxWEa8c"
   },
   "outputs": [],
   "source": [
    "ansr = final_bot.run_chain(\"what are sunspots\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q8TNh8sNEa6G",
    "outputId": "677edfa1-456c-4b6a-df02-d89a29b94e51"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'what are sunspots',\n",
       " 'response': 'Sunspots form on the surface of the Sun due to strong magnetic field lines coming up from within the Sun trough the solar surface and appear visibly as dark spots compared to their surroundings.',\n",
       " 'has_media': True,\n",
       " 'images': [{'path': './images_tables/images/image_29.jpg', 'description': ''},\n",
       "  {'path': './images_tables/images/image_30.jpg', 'description': ''}],\n",
       " 'tables': []}"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ansr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C-QMuIOeEa1S"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
